# -*- coding: utf-8 -*-
# è®­ç»ƒåŸºäºè´Ÿé‡‡æ ·çš„è¯åµŒå…¥æ¨¡å‹ï¼ˆWord2Vec Skip-gramï¼‰
import re
import math
import numpy as np
import random
import torch
import torch.utils.data as tud
import torch.optim as optim
from tqdm import tqdm
from collections import Counter
from model import EmbeddingModel
from preprocess import WordEmbeddingDataset

# device = "cuda" if torch.cuda.is_available() else 'cpu'
device = torch.device("cuda:2" if torch.cuda.is_available() else "cpu")
EPOCHS = 100
MAX_VOCAB_SIZE = 10000
EMBEDDING_SIZE = 200
BATCH_SIZE = 512
LR = 0.001
TRAIN_DATA_PATH = 'text8_toy.txt'
OUT_DIR = './result_example'

with open(TRAIN_DATA_PATH) as f:
    text = f.read()

# ä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€æ’‡å·ã€è¿å­—ç¬¦
text = re.sub(r"[^a-z0-9\s'-]", '', text.lower())
words = text.split()

# å­é‡‡æ ·ï¼ˆSubsampling Frequent Wordsï¼‰
vocab_count_ = dict(Counter(words))
total_count = sum(vocab_count_.values())

t = 1e-5
subsampling = []
for word in words:
    # ä¸¢å¼ƒé«˜é¢‘è¯
    f = vocab_count_[word] / total_count # è¯é¢‘
    P_drop = 1 - math.sqrt(t / f) if f > t else 0
    if random.random() > P_drop:
        subsampling.append(word)

# ğŸŒ¿ğŸŒ¿ğŸŒ¿ è¯å‘é‡è®­ç»ƒä¸­æ„å»ºè¯æ±‡è¡¨ï¼ˆvocabularyï¼‰
# â‘ return:  Counter({'the': 1000, 'cat': 500, 'dog': 450, ..., 'rare_word': 1})
# â‘¡è¿”å›ä¸€ä¸ªåˆ—è¡¨
# â‘¢å°†åˆ—è¡¨è½¬ä¸º å­—å…¸
vocab_count = dict(Counter(subsampling).most_common(MAX_VOCAB_SIZE - 1))
# unknownï¼šä»£è¡¨æ‰€æœ‰æœªç™»å½•è¯ ï¼ˆ9999ï¼šçœŸå®è¯ï¼›1ï¼šæœªç™»å½•è¯ï¼‰ï¼šä½é¢‘è¯æˆ–è€…è®­ç»ƒæ—¶æ²¡è§è¿‡çš„è¯æˆ–è€…æ‹¼å†™é”™è¯¯
vocab_count['<UNK>'] = 1

idx2word = [word for word in vocab_count.keys()]
word2idx = {word: i for i, word in enumerate(idx2word)}

# è´Ÿé‡‡æ ·åˆ†å¸ƒï¼ˆUnigram ^ 3/4ï¼‰ï¼šç”¨è¯é¢‘çš„ 3/4 æ¬¡æ–¹å½’ä¸€åŒ–ä½œä¸º  é‡‡æ ·æ¦‚ç‡ã€‚å¹‚æ¬¡å¹³æ»‘ä»¥é™ä½é«˜é¢‘è¯çš„é‡‡æ ·æ¦‚ç‡ã€‚
nc = np.array([count for count in vocab_count.values()], dtype=np.float32)** (3./4.)
word_freqs = nc / np.sum(nc)

dataset = WordEmbeddingDataset(subsampling, word2idx, word_freqs)

dataloader = tud.DataLoader(dataset, BATCH_SIZE, shuffle=True)

model = EmbeddingModel(len(idx2word), EMBEDDING_SIZE)
model.to(device)
model.train()
optimizer = optim.Adam(model.parameters(), lr=LR)


# é€šè¿‡å¤šè½®è®­ç»ƒï¼ˆepochsï¼‰ï¼Œè®©æ¨¡å‹å­¦ä¼šå°† è¯­ä¹‰ç›¸è¿‘çš„è¯åœ¨å‘é‡ç©ºé—´ä¸­æ‹‰è¿‘ï¼Œä¸ç›¸å…³çš„è¯æ¨è¿œã€‚
# ä½¿ç”¨çš„æŠ€æœ¯æ˜¯ï¼šSkip-gram + è´Ÿé‡‡æ ·ï¼ˆNegative Samplingï¼‰
for epoch in range(EPOCHS):
    pbar = tqdm(dataloader) # DataLoader å¯¹è±¡ï¼Œæ¯æ¬¡è¿”å›ä¸€ä¸ªbatchçš„æ•°æ®
    pbar.set_description("[Epoch {}]".format(epoch))
    # print(len(dataloader))
    for i, (input_labels, pos_labels, neg_labels) in enumerate(pbar):
        # æ¯æ¬¡å–å‡ºä¸€ä¸ª batch çš„ä¸‰å…ƒç»„
        input_labels = input_labels.to(device) # è¾“å…¥è¯ï¼ˆä¸­å¿ƒè¯ï¼‰ [512,]
        pos_labels = pos_labels.to(device)  # æ­£æ ·æœ¬ä¸Šä¸‹æ–‡è¯ï¼ˆçœŸå®ä¸Šä¸‹æ–‡è¯ï¼‰[512,4]
        neg_labels = neg_labels.to(device)  # è´Ÿæ ·æœ¬è¯ ï¼ˆéšæœºé‡‡æ ·çš„éä¸Šä¸‹æ–‡è¯ï¼‰[512,15]
        # optimizer.zero_grad() # ç­‰ä»·
        model.zero_grad()
        # å‰å‘ä¼ æ’­ï¼ˆè®¾batch_size = 64,é‚£ä¹ˆ loss æ˜¯ä¸€ä¸ªåŒ…å« 64ä¸ªå€¼çš„å¼ é‡ï¼Œ.mean()å¾—åˆ°ä¸€ä¸ªæ ‡é‡ï¼‰ï¼Œå°†ä¸€æ‰¹æ ·æœ¬çš„æŸå¤±â€œå‹ç¼©â€æˆä¸€ä¸ªæ•°ï¼Œä»¥ä¾¿ PyTorch èƒ½æ­£ç¡®è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
        loss = model(input_labels, pos_labels, neg_labels).mean()
        loss.backward()
        optimizer.step()
        # å°† Tensor ä¸­çš„æ ‡é‡è½¬æ¢ä¸º Python æ•°å€¼
        pbar.set_postfix(loss=loss.item())

model.save_embedding(OUT_DIR, idx2word) 